{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Step 1: Import Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Install necessary libraries\n",
    "!pip3 install python-terrier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import necessary libraries\n",
    "from tira.third_party_integrations import ensure_pyterrier_is_loaded, persist_and_normalize_run\n",
    "from tira.rest_api_client import Client\n",
    "\n",
    "import nltk\n",
    "\n",
    "import pandas as pd\n",
    "import pyterrier as pt\n",
    "from pyterrier.measures import *\n",
    "import math"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a REST client to the TIRA platform for retrieving the pre-indexed data.\n",
    "ensure_pyterrier_is_loaded()\n",
    "tira = Client()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize PyTerrier\n",
    "if not pt.started():\n",
    "    pt.init()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# The dataset: the union of the IR Anthology and the ACL Anthology\n",
    "# This line creates an IRDSDataset object and registers it under the name provided as an argument.\n",
    "dataset = pt.get_dataset('irds:ir-lab-sose-2024/ir-acl-anthology-20240504-training')\n",
    "\n",
    "# A (pre-built) PyTerrier index loaded from TIRA\n",
    "index = tira.pt.index('ir-lab-sose-2024/tira-ir-starter/Index (tira-ir-starter-pyterrier)', dataset)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Step 2: Data Loading"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Display some example data\n",
    "topics = dataset.get_topics('text')\n",
    "qrels = dataset.get_qrels()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(topics.head())\n",
    "print(qrels.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 3: Data Preprocessing\n",
    "# Here we would include steps to clean and preprocess the text, such as removing stop words, stemming, etc.\n",
    "# For this example, we'll assume the data is already preprocessed."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Step 4: Information Retrieval Models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# BM25\n",
    "bm25 = pt.BatchRetrieve(index, wmodel=\"BM25\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TF-IDF\n",
    "tfidf = pt.BatchRetrieve(index, wmodel=\"TF_IDF\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Step 5: Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluate BM25\n",
    "bm25_results = bm25.transform(topics)\n",
    "bm25_metrics = pt.Experiment(\n",
    "    [bm25],\n",
    "    topics,\n",
    "    qrels,\n",
    "    eval_metrics=[\"map\", \"ndcg\", \"recip_rank\"]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluate TF-IDF\n",
    "tfidf_results = tfidf.transform(topics)\n",
    "tfidf_metrics = pt.Experiment(\n",
    "    [tfidf],\n",
    "    topics,\n",
    "    qrels,\n",
    "    eval_metrics=[\"map\", \"ndcg\", \"recip_rank\"]\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Step 6: Conclusion"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "run = bm25_results\n",
    "persist_and_normalize_run(run, system_name='bm25-baseline', default_output='../runs')\n",
    "\n",
    "run = tfidf_results\n",
    "persist_and_normalize_run(run, system_name='tfidf-baseline', default_output='../runs')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Summarize findings\n",
    "print(\"BM25 Metrics:\", bm25_metrics)\n",
    "print(\"TF-IDF Metrics:\", tfidf_metrics)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Step 7 : Benchmarks"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### bm25"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Local test of BM25\n",
    "bm25_experiment = pt.Experiment(\n",
    "    [bm25],\n",
    "    topics,\n",
    "    qrels,\n",
    "    eval_metrics=['P_1000', 'map', 'recip_rank'],\n",
    "    names=['BM25'],\n",
    "    baseline=0\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Display BM25 results\n",
    "print(\"BM25 Experiment Results\")\n",
    "print(bm25_experiment)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### TF-IDF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Local test of TF-IDF\n",
    "tfidf_experiment = pt.Experiment(\n",
    "    [tfidf],\n",
    "    topics,\n",
    "    qrels,\n",
    "    eval_metrics=['P_1000', 'map', 'recip_rank'],\n",
    "    names=['TF-IDF'],\n",
    "    baseline=0\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Display TF-IDF results\n",
    "print(\"TF-IDF Experiment Results\")\n",
    "print(tfidf_experiment)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
